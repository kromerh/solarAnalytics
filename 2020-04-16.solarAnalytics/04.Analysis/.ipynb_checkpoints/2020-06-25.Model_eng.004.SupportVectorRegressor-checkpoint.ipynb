{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model engineering 004: Support Vector Regression\n",
    "\n",
    "In this part of the project to predict the photovoltaic production of solar cells on a roof we are considering a Support Vector regression model. \n",
    "\n",
    "We will treat this as a regression problem, not taking the temporal aspect, i.e. time series forcasting, into account. Data mining and missing value treatment of weather data from the DarkSky API and data from the photovoltaic system were covered in other notebooks:\n",
    "\n",
    "- Data mining and EDA of weather data: https://kyso.io/heiko/predicting-solar-panel-output-eda-of-photovoltaic-data\n",
    "\n",
    "- Data mining and EDA of photovoltaic data: https://kyso.io/heiko/predicting-solar-panel-output-eda-of-weather-data\n",
    "\n",
    "- Missing value treatment: https://kyso.io/heiko/predicting-solar-panel-output-missing-value-treatment-of-weather-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "\n",
    "We will apply support vector regression by following these steps:\n",
    "\n",
    "1. Load the data into one dataframe\n",
    "\n",
    "2. Select the features we will use for the prediction. We can look at the correlation matrix and remove redundant features that are correlated. Multicollinearity undermines the statistical significance of an independent variable. While it should not have a major impact on the model’s accuracy, it does affect the variance associated with the prediction, as well as, reducing the quality of the interpretation of the independent variables. In other words, the effect your data has on the model isn’t trustworthy. Your explanation of how the model takes the inputs to produce the output will not be reliable. (You can read more about this here: https://towardsdatascience.com/multicollinearity-why-is-it-a-problem-398b010b77ac). We will just drop (one of the) columns that are correlated. We could feature engineer another features that combines the correlated features, but at this point this will not be considered.\n",
    "\n",
    "3. Consider missing values. This is still part of the feature selection process. We remove features that have lots of missing values that we could not interpolate.\n",
    "\n",
    "4. Model selection. We will use RandomizedSearch and GridSearch approach to tune hyperparameters of the SupportVectorRegression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import eli5\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset\n",
    "\n",
    "This is the cleaned dataset from the MultiLinear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../02.Prepared_data/dataset.Model_eng.001.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection\n",
    "\n",
    "Select the features in a pipeline as for the linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df_cleaned.iloc[:, 1:].values\n",
    "X = data.iloc[:, 1:]\n",
    "X = X.reset_index(drop=True)\n",
    "# y = df_cleaned.iloc[:, 0].values\n",
    "y = data.iloc[:, 0]\n",
    "y = y.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numeric features: StandardScaler\n",
    "\n",
    "Standardize features by removing the mean and scaling to unit variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = ['apparentTemperatureHigh', 'precipProbability', 'uvIndex', 'precipIntensityMax_cm', 'sun_uptime']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical features: OneHotEncoder\n",
    "\n",
    "This preprocessing step will encode the `season` and `precipType` column values into a vector of length 3. For example, winter will be encoded as [1,0,0], summer as [0,1,0], and so on for the other seasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['season', 'precipType']\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder())])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', SVR())])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomized and GridSearch to tune the hyperparameters\n",
    "\n",
    "SVRs are easily adaptable and work well on non linear problems and are generally not biased by outliers. However, they are more difficult to understand compared to other models.\n",
    "\n",
    "In an SVR model, a tube is fit to the dataset with a margin that is considered as epsilon. Points outside of this tube dictate how the tube (and hence prediction) look like. That is why they are called support vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['memory',\n",
       " 'steps',\n",
       " 'verbose',\n",
       " 'preprocessor',\n",
       " 'classifier',\n",
       " 'preprocessor__n_jobs',\n",
       " 'preprocessor__remainder',\n",
       " 'preprocessor__sparse_threshold',\n",
       " 'preprocessor__transformer_weights',\n",
       " 'preprocessor__transformers',\n",
       " 'preprocessor__verbose',\n",
       " 'preprocessor__num',\n",
       " 'preprocessor__cat',\n",
       " 'preprocessor__num__memory',\n",
       " 'preprocessor__num__steps',\n",
       " 'preprocessor__num__verbose',\n",
       " 'preprocessor__num__scaler',\n",
       " 'preprocessor__num__scaler__copy',\n",
       " 'preprocessor__num__scaler__with_mean',\n",
       " 'preprocessor__num__scaler__with_std',\n",
       " 'preprocessor__cat__memory',\n",
       " 'preprocessor__cat__steps',\n",
       " 'preprocessor__cat__verbose',\n",
       " 'preprocessor__cat__onehot',\n",
       " 'preprocessor__cat__onehot__categories',\n",
       " 'preprocessor__cat__onehot__drop',\n",
       " 'preprocessor__cat__onehot__dtype',\n",
       " 'preprocessor__cat__onehot__handle_unknown',\n",
       " 'preprocessor__cat__onehot__sparse',\n",
       " 'classifier__C',\n",
       " 'classifier__cache_size',\n",
       " 'classifier__coef0',\n",
       " 'classifier__degree',\n",
       " 'classifier__epsilon',\n",
       " 'classifier__gamma',\n",
       " 'classifier__kernel',\n",
       " 'classifier__max_iter',\n",
       " 'classifier__shrinking',\n",
       " 'classifier__tol',\n",
       " 'classifier__verbose']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first access the parameter keys of the individual estimators\n",
    "list(clf.get_params().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done 223 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done 277 out of 300 | elapsed:    4.0s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:    7.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=Pipeline(steps=[('preprocessor',\n",
       "                                              ColumnTransformer(transformers=[('num',\n",
       "                                                                               Pipeline(steps=[('scaler',\n",
       "                                                                                                StandardScaler())]),\n",
       "                                                                               ['apparentTemperatureHigh',\n",
       "                                                                                'precipProbability',\n",
       "                                                                                'uvIndex',\n",
       "                                                                                'precipIntensityMax_cm',\n",
       "                                                                                'sun_uptime']),\n",
       "                                                                              ('cat',\n",
       "                                                                               Pipeline(steps=[('onehot',\n",
       "                                                                                                OneHotEncoder())]),\n",
       "                                                                               ['season',\n",
       "                                                                                'precipType'])])),\n",
       "                                             ('classifier', SVR())]),\n",
       "                   n_iter=100, n_jobs=-1,\n",
       "                   param_distributions={'classifier__C': [0.0001, 0.01, 0.1, 1,\n",
       "                                                          10, 100, 1000,\n",
       "                                                          10000],\n",
       "                                        'classifier__epsilon': [0.0001, 0.001,\n",
       "                                                                0.01, 0.1, 1,\n",
       "                                                                10],\n",
       "                                        'classifier__kernel': ['linear', 'poly',\n",
       "                                                               'rbf', 'sigmoid',\n",
       "                                                               'precomputed'],\n",
       "                                        'classifier__tol': [0.001, 0.0001,\n",
       "                                                            1e-05]},\n",
       "                   random_state=42, verbose=2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Kernel\n",
    "kernel = ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed']\n",
    "# Tolerance for stopping criterion\n",
    "tol = [1e-3, 1e-4, 1e-5]\n",
    "# Regularization parameter.\n",
    "C = [0.0001, 0.01, 0.1, 1, 10, 100, 1000, 10000]\n",
    "# Epsilon in the epsilon-SVR model.\n",
    "epsilon = [0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
    "\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'classifier__kernel': kernel,\n",
    "               'classifier__tol': tol,\n",
    "               'classifier__C': C,\n",
    "               'classifier__epsilon': epsilon}\n",
    "\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "# clf = RandomForestClassifier()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "clf_random = RandomizedSearchCV(estimator=clf, param_distributions=random_grid, n_iter=100, cv=3, verbose=2, random_state=42, n_jobs=-1)\n",
    "# Fit the random search model\n",
    "clf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier__tol': 0.0001,\n",
       " 'classifier__kernel': 'rbf',\n",
       " 'classifier__epsilon': 0.001,\n",
       " 'classifier__C': 10}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the RBF (radial basis function, gaussian) is the best kernel for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 90 candidates, totalling 270 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 270 out of 270 | elapsed:    1.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=Pipeline(steps=[('preprocessor',\n",
       "                                        ColumnTransformer(transformers=[('num',\n",
       "                                                                         Pipeline(steps=[('scaler',\n",
       "                                                                                          StandardScaler())]),\n",
       "                                                                         ['apparentTemperatureHigh',\n",
       "                                                                          'precipProbability',\n",
       "                                                                          'uvIndex',\n",
       "                                                                          'precipIntensityMax_cm',\n",
       "                                                                          'sun_uptime']),\n",
       "                                                                        ('cat',\n",
       "                                                                         Pipeline(steps=[('onehot',\n",
       "                                                                                          OneHotEncoder())]),\n",
       "                                                                         ['season',\n",
       "                                                                          'precipType'])])),\n",
       "                                       ('classifier', SVR())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'classifier__C': [0.0001, 0.01, 0.1, 1, 10, 100],\n",
       "                         'classifier__epsilon': [1e-05, 0.0001, 0.001, 0.01,\n",
       "                                                 0.1],\n",
       "                         'classifier__kernel': ['rbf'],\n",
       "                         'classifier__tol': [0.001, 0.0001, 1e-05]},\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Kernel\n",
    "kernel = ['rbf']\n",
    "# Tolerance for stopping criterion\n",
    "tol = [1e-3, 1e-4, 1e-5]\n",
    "# Regularization parameter.\n",
    "C = [0.0001, 0.01, 0.1, 1, 10, 100]\n",
    "# Epsilon in the epsilon-SVR model.\n",
    "epsilon = [0.00001, 0.0001, 0.001, 0.01, 0.1]\n",
    "\n",
    "\n",
    "# Create the random grid\n",
    "param_grid = {'classifier__kernel': kernel,\n",
    "               'classifier__tol': tol,\n",
    "               'classifier__C': C,\n",
    "               'classifier__epsilon': epsilon}\n",
    "\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, \n",
    "                          cv=3, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(transformers=[('num',\n",
       "                                                  Pipeline(steps=[('scaler',\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  ['apparentTemperatureHigh',\n",
       "                                                   'precipProbability',\n",
       "                                                   'uvIndex',\n",
       "                                                   'precipIntensityMax_cm',\n",
       "                                                   'sun_uptime']),\n",
       "                                                 ('cat',\n",
       "                                                  Pipeline(steps=[('onehot',\n",
       "                                                                   OneHotEncoder())]),\n",
       "                                                  ['season', 'precipType'])])),\n",
       "                ('classifier', SVR(C=10))])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = grid_search.best_estimator_\n",
    "best_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model: r2 score and MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7944537346006463"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = best_model.predict(X_test)\n",
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40.02518288147158"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "We applied Support Vector Regression to the dataset, predicting the solar panel output values for different weather conditions.\n",
    "For the model with an rbf kernel, we found a MSE (root mean squared error) of 40.0 and an r2_score of 0.79.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
