{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for Time Series Forecasting\n",
    "\n",
    "Sources: \n",
    "- https://machinelearningmastery.com/how-to-get-started-with-deep-learning-for-time-series-forecasting-7-day-mini-course/\n",
    "- Machine Learning with Python Cookbook by Chris Albon\n",
    "- Talk by Ilja Rasin (https://www.linkedin.com/in/iljarasin/) at IBM Developer Unconference June 2019, Switzerland\n",
    "\n",
    "I found this course via LinkedIn posted by Steve Nouri (https://www.linkedin.com/in/stevenouri/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 01: Promise of Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, you will discover the promise of deep learning methods for time series forecasting. <br>\n",
    "Generally, neural networks like Multilayer Perceptrons or MLPs provide capabilities that are offered by few algorithms, such as:<br>\n",
    " - Robust to Noise. Neural networks are robust to noise in input data and in the mapping function and can even support learning and prediction in the presence of missing values.\n",
    " - Nonlinear. Neural networks do not make strong assumptions about the mapping function and readily learn linear and nonlinear relationships.\n",
    " - Multivariate Inputs. An arbitrary number of input features can be specified, providing direct support for multivariate forecasting.\n",
    " - Multi-step Forecasts. An arbitrary number of output values can be specified, providing direct support for multi-step and even multivariate forecasting.\n",
    "\n",
    "#### From Machine Learning with Python Cookbook \n",
    "> Multilayer perceptrons are feedforward neural networks and represent the simplest artificial neural network used in any real-world setting. The name feedforward comes from the fact that the observations feature values are fed forward through the network. Each layer aims to transform the feature values so that the output at the end is the same as the target's value. \n",
    "\n",
    "> Forward propagagion means that an observation (usually a set of observations called a batch) is fed through the network and the output is compared with the true value of the observation using a loss function. \n",
    "\n",
    "> Backward propagation means that after the forward propagation the algorithm goes back through the network identifying how much each parameter has contributed to the error between predicted and true value. The optimization algorithm determines at each parameter how much each weight should be adjusted to improve the output.\n",
    "\n",
    "> The way neural networks learn is by repeating this process of forward and backpropagation for every observation multiple times. Each time all observations have been sent through the network is called an epoch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your Task\n",
    "\n",
    "For this lesson you must suggest one capability from both Convolutional Neural Networks and Recurrent Neural Networks that may be beneficial in modeling time series forecasting problems.\n",
    "\n",
    "Post your answer in the comments below. I would love to see what you discover.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "\n",
    "##### Convolutional Neural Networks (CNN):\n",
    "    Sources: \n",
    "        - https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/\n",
    "        \n",
    "CNNs have proven to be very effective in areas such as image recognition and classification. It takes an imput image, assigns importance, meaning that it learns weights \n",
    "and biases. The architecture of a CNN mimics the connectivity pattern of Neurons in the Human Brain. It is dense, meaning that \"everything is connected with everything\" and the resulting network is dense. Typically, a CNN consists of several layers with four main operations (typically there is a sequence of many CONV, RELU, CONV, RELU, POOL, CONV, RELU, VON, RELU, POOL, ...):\n",
    "    \n",
    "1. Convolution\n",
    "    Extract features from the input image using a feature detector (kernel). The resulting (filtered) image is called a Feature Map. In a CNN there are typically many different type of filters producing a Feature Map with a certain depth.\n",
    "2. ReLU (Rectified Linear Unit)\n",
    "    Introduces Non Linearity, by adding an additional element wise operation (on each pixel) after every Convolution operation. It replaces all negative pixel values in the Feature Map with zero.\n",
    "3. Pooling or Sub Sampling\n",
    "    Reduces the dimensionality of each Feature Map while retaining the most important information. Spatial pooling is usually Max, Average, Sum, ... For example in Max Pooling, a submatrix of the Feature Map is selected (i.e. 2x2) and take the largest element from the rectified feature map within this submatrix. Pooling progressively reduces spatial size of the input representation. It reduces dimensionality, also reduces number of parameters and computations in the network as a means to control overfitting. It also makes the network invariant to small transformations and help to arrive at an almost scale invariant representation of the image\n",
    "4. Classification\n",
    "    In the last step, a MLP is used in combination with i.e. a Softmax activation function to output probabilities, i.e. what is displayed in the image. \n",
    "        \n",
    "\n",
    "#### Recurrent Neural Networks (RNN) \n",
    "    Sources:\n",
    "        -https://machinelearningmastery.com/promise-recurrent-neural-networks-time-series-forecasting/\n",
    "RNNS (like LSTM) allow the explicit handling of order between observations when learning a mapping function from inputs to outputs. What this means is that there can be a layout of the network such that the connection between input layer and encoder layer 1 is dense, the connection within the network are recurrent, i.e. not dense, and the connection between decoder and output layer is again dense. That way the encoder layer learns to \"read\" the data and puts out a prediction, but the decoder layer does not consider the output of the encoder, it only considers the \"thought\" process that the encoder underwent and takes it from there.\n",
    "\n",
    "The promise of recurrent neural networks is that the temporal dependence in the input data can be learned. That a fixed set of lagged observations does not need to be specified. Rather than having a single multi-tasking cell, the model will use two specialised cells. One for memorising important events of the past (encoder) and one for converting the important events into a prediction of the future (decoder).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
